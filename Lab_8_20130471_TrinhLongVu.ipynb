{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trinhvu1711/ML_Semester2_2023/blob/main/Lab_8_20130471_TrinhLongVu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMzehe0sy5wr"
      },
      "source": [
        "# This lab deals with **GridSearchCV** for tuning the hyper-parameters of an estimator and applying vectorization techniques to the **movie reviews dataset** for classification task. \n",
        "\n",
        "*   **Deadline: 23:59, 17/4/2023**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4nJmxp9zGX4"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DoVWQ8AEyc-C"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import recall_score \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ITCmwS5jow"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "# print(iris)\n",
        "# X, y without Feature Selection.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dG9SA5OhGT"
      },
      "source": [
        "#Task 1. With **iris** dataset\n",
        "*  1.1. Apply **GridSearchCV** for **SVM** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62jExOZ952fF",
        "outputId": "61012d98-e687-40c2-91d1-7ee3d56a48cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      1.00      1.00        17\n",
            "           2       1.00      1.00      1.00        14\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_gamma', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
            "SVC(C=1, gamma=1, kernel='linear')\n",
            "0.980952380952381\n",
            "Accuracy :  1.0\n",
            "Precision :  1.0\n",
            "Recall score :  1.0\n",
            "F1 score :  1.0\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "predictions = svc.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(SVC(), param_grid, refit=True, scoring = 'accuracy')\n",
        "clf.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "y_pred = svc.predict(X_test)\n",
        "svmac = round(accuracy_score(y_test, y_pred), 2)\n",
        "svmpc = round(precision_score(y_test, y_pred, average='micro'), 2)\n",
        "svmrc= round(recall_score(y_test, y_pred, average='micro'), 2)\n",
        "svmf= round(f1_score(y_test, y_pred, average='micro'), 2)\n",
        "print (\"Accuracy : \", svmac) \n",
        "print (\"Precision : \", svmpc) \n",
        "print (\"Recall score : \", svmrc) \n",
        "print (\"F1 score : \", svmf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g--8cng53sY"
      },
      "source": [
        "*  1.2. Apply **GridSearchCV** for **kNN** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "```\n",
        "where\n",
        "\n",
        "    *  **n_neighbors**: Decide the best k based on the values we have computed earlier.\n",
        "    *  **weights**: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\n",
        "    *  **metric**: The distance metric to be used will calculating the similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX0_kItYPism",
        "outputId": "960908d1-ba25-479f-8de9-979429ab84c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      0.94      0.97        17\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_metric', 'param_n_neighbors', 'param_weights', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'distance'}\n",
            "KNeighborsClassifier(weights='distance')\n",
            "0.9619047619047619\n",
            "kNN Classifier:\n",
            "Accuracy: 0.9778\n",
            "Precision: 0.9778\n",
            "Recall: 0.9778\n",
            "F1-score: 0.9778\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code#code\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(KNeighborsClassifier(), grid_params,scoring = 'accuracy', verbose = 1, cv=3, n_jobs = -1, refit = True)\n",
        "clf.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "y_pred_knn =  knn.predict(X_test)\n",
        "knnac =round(accuracy_score(y_test, y_pred_knn), 4)\n",
        "knnpc =round(precision_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnrc=round(recall_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnf=round(f1_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "print('kNN Classifier:')\n",
        "print('Accuracy:', knnac)\n",
        "print('Precision:', knnpc)\n",
        "print('Recall:', knnrc)\n",
        "print('F1-score:', knnf)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQSOcjL_TIW"
      },
      "source": [
        "*  1.3. Apply **GridSearchCV** for **Random Forest** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlyF9WpN_01p",
        "outputId": "842b75c0-494e-4f2d-a1cd-241f0b19e042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      0.94      0.97        17\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_metric', 'param_n_neighbors', 'param_weights', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'uniform'}\n",
            "KNeighborsClassifier()\n",
            "0.98\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "rfcsf=RandomForestClassifier()\n",
        "#Train the model using the training sets\n",
        "rfcsf.fit(X_train,y_train)\n",
        "y_pred = rfcsf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "CV_rfc = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv= 5)\n",
        "CV_rfc.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_eaCJiXDa2J",
        "outputId": "d82f890e-fc48-4d07-a57f-a86694c8968f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  0.9778\n",
            "Precision :  0.9778\n",
            "Recall score :  0.9778\n",
            "F1 score :  0.9778\n"
          ]
        }
      ],
      "source": [
        "rfac = round(accuracy_score(y_test, y_pred), 4)\n",
        "rfpc = round(precision_score(y_test, y_pred, average='micro'), 4)\n",
        "rfrc= round(recall_score(y_test, y_pred, average='micro'), 4)\n",
        "rff= round(f1_score(y_test, y_pred, average='micro'), 4)\n",
        "print (\"Accuracy : \", rfac) \n",
        "print (\"Precision : \", rfpc) \n",
        "print (\"Recall score : \", rfrc) \n",
        "print (\"F1 score : \", rff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3N7TD7s_3Kp"
      },
      "source": [
        "*   1.4 Compare the best obtained results from 1.1 to 1.3 (use PrettyTable to dispaly the results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmECdm8-EDAK",
        "outputId": "41c08daf-ec65-48da-d66c-c3d8715f1728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------+----------+-----------+--------+----------+\n",
            "| Classification algorithm | Accuracy | Precision | Recall | F1-score |\n",
            "+--------------------------+----------+-----------+--------+----------+\n",
            "|      Random Forest       |  0.9778  |   0.9778  | 0.9778 |  0.9778  |\n",
            "|           KNN            |  0.9778  |   0.9778  | 0.9778 |  0.9778  |\n",
            "|           SVM            |   1.0    |    1.0    |  1.0   |   1.0    |\n",
            "+--------------------------+----------+-----------+--------+----------+\n"
          ]
        }
      ],
      "source": [
        "table1 = PrettyTable()\n",
        "table1.field_names = [\"Classification algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        "table1.add_row([\"Random Forest\", rfac, rfpc, rfrc, rff])\n",
        "table1.add_row([\"KNN\", knnac, knnpc, knnrc, knnf])\n",
        "table1.add_row([\"SVM\",svmac, svmpc, svmrc, svmf ])\n",
        "table1.title = 'Results using GridSearchCV'\n",
        "print(table1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNv07ARGzOUm"
      },
      "source": [
        "#Task 2. \n",
        "For breast cancer dataset (https://tinyurl.com/3vme8hr3) which could be loaded from datasets in sklearn as follows:\n",
        "\n",
        "```\n",
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "```\n",
        "\n",
        "*   Apply **GridSearchCV** to different classification algorithms such as **SVM, kNN, LogisticRegression, RandomForest**.\n",
        "*   Compare the results obtained by the best hyperparameters among classification algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sM2gn8L7EmoJ"
      },
      "outputs": [],
      "source": [
        "# Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MN7oLqTdEoEp"
      },
      "outputs": [],
      "source": [
        "X = cancer['data']\n",
        "y = cancer['target']\n",
        "# print(iris)\n",
        "# X, y without Feature Selection.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnoVB8J4vV36"
      },
      "source": [
        "*   2.1. Apply **GridSearchCV** to **SVM** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZTSvsJdvYqI",
        "outputId": "d2e65a46-6b4c-41cb-a662-98710cf27017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        59\n",
            "           1       1.00      0.97      0.99       112\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.98      0.99      0.98       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_gamma', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "SVC(C=1000, gamma=0.0001)\n",
            "0.9824050632911392\n",
            "Accuracy :  0.98\n",
            "Precision :  0.98\n",
            "Recall score :  0.98\n",
            "F1 score :  0.98\n"
          ]
        }
      ],
      "source": [
        "# code\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "predictions = svc.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(SVC(), param_grid, refit=True, scoring = 'accuracy')\n",
        "clf.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "y_pred = svc.predict(X_test)\n",
        "svmac = round(accuracy_score(y_test, y_pred), 2)\n",
        "svmpc = round(precision_score(y_test, y_pred, average='micro'), 2)\n",
        "svmrc= round(recall_score(y_test, y_pred, average='micro'), 2)\n",
        "svmf= round(f1_score(y_test, y_pred, average='micro'), 2)\n",
        "print (\"Accuracy : \", svmac) \n",
        "print (\"Precision : \", svmpc) \n",
        "print (\"Recall score : \", svmrc) \n",
        "print (\"F1 score : \", svmf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol1U_T_NvcqV"
      },
      "source": [
        "*   2.2. Apply **GridSearchCV** to **kNN** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt71yrAoBwYE",
        "outputId": "ea013eec-5d0a-4c39-a52c-4eb6b6479c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97        59\n",
            "           1       0.98      0.99      0.99       112\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.98      0.98      0.98       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_metric', 'param_n_neighbors', 'param_weights', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'metric': 'minkowski', 'n_neighbors': 7, 'weights': 'distance'}\n",
            "KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
            "0.9573175362649047\n",
            "kNN Classifier:\n",
            "Accuracy: 0.9825\n",
            "Precision: 0.9825\n",
            "Recall: 0.9825\n",
            "F1-score: 0.9825\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "#code#code\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(KNeighborsClassifier(), grid_params,scoring = 'accuracy', verbose = 1, cv=3, n_jobs = -1, refit = True)\n",
        "clf.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "y_pred_knn =  knn.predict(X_test)\n",
        "knnac =round(accuracy_score(y_test, y_pred_knn), 4)\n",
        "knnpc =round(precision_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnrc=round(recall_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnf=round(f1_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "print('kNN Classifier:')\n",
        "print('Accuracy:', knnac)\n",
        "print('Precision:', knnpc)\n",
        "print('Recall:', knnrc)\n",
        "print('F1-score:', knnf)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPkAvse-BxNa"
      },
      "source": [
        "*   2.3. Apply **GridSearchCV** to **LogisticRegression** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyYjpHFbB1Ci",
        "outputId": "9e5245ac-30cf-46a1-bda8-7041d244e0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1}\n",
            "LogisticRegression(C=1)\n",
            "KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
            "0.9573175362649047\n",
            "Logistic Regression Classifier:\n",
            "Accuracy: 0.96\n",
            "Precision: 0.98\n",
            "Recall: 0.96\n",
            "F1-score: 0.97\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "clf_lr = LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, refit= True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "sorted(grid_search.cv_results_.keys())\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid_search.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid_search.best_estimator_)\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "y_pred_lr = clf_lr.predict(X_test)\n",
        "lrac = round(accuracy_score(y_test, y_pred_lr), 2)\n",
        "lrpc = round(precision_score(y_test, y_pred_lr), 2)\n",
        "lrrc= round(recall_score(y_test, y_pred_lr), 2)\n",
        "lrf= round(f1_score(y_test, y_pred_lr), 2)\n",
        "print('Logistic Regression Classifier:')\n",
        "print('Accuracy:', lrac)\n",
        "print('Precision:', lrpc)\n",
        "print('Recall:', lrrc)\n",
        "print('F1-score:', lrf)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NjSLo5jB1xY"
      },
      "source": [
        "*   2.4. Apply **GridSearchCV** to **RandomForest** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nktGtM0PB7XB",
        "outputId": "4db8ecdd-c427-4817-c2a1-4c927e501fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        59\n",
            "           1       1.00      0.97      0.99       112\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.98      0.99      0.98       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_metric', 'param_n_neighbors', 'param_weights', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'uniform'}\n",
            "KNeighborsClassifier(metric='manhattan', n_neighbors=7)\n",
            "0.9371724766461608\n",
            "Accuracy :  0.9825\n",
            "Precision :  0.9825\n",
            "Recall score :  0.9825\n",
            "F1 score :  0.9825\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "#code\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "rfcsf=RandomForestClassifier()\n",
        "#Train the model using the training sets\n",
        "rfcsf.fit(X_train,y_train)\n",
        "y_pred = rfcsf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "CV_rfc = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv= 5)\n",
        "CV_rfc.fit(X_train, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "rfac = round(accuracy_score(y_test, y_pred), 4)\n",
        "rfpc = round(precision_score(y_test, y_pred, average='micro'), 4)\n",
        "rfrc= round(recall_score(y_test, y_pred, average='micro'), 4)\n",
        "rff= round(f1_score(y_test, y_pred, average='micro'), 4)\n",
        "print (\"Accuracy : \", rfac) \n",
        "print (\"Precision : \", rfpc) \n",
        "print (\"Recall score : \", rfrc) \n",
        "print (\"F1 score : \", rff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJ3BSHpB9Dx"
      },
      "source": [
        "*   2.5. Compare the best obtained results among classification algorithms (use PrettyTable to dispaly the results) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LS_IYfNCFEj",
        "outputId": "b06976ac-4117-4159-9913-e2845c5c7067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+----------+-----------+--------+----------+\n",
            "| Classification algorithm | Accuracy | Precision | Recall | F1-score |\n",
            "+--------------------------+----------+-----------+--------+----------+\n",
            "|      Random Forest       |  0.9825  |   0.9825  | 0.9825 |  0.9825  |\n",
            "|           KNN            |  0.9825  |   0.9825  | 0.9825 |  0.9825  |\n",
            "|           SVM            |   0.98   |    0.98   |  0.98  |   0.98   |\n",
            "|    LogisticRegression    |   0.96   |    0.98   |  0.96  |   0.97   |\n",
            "+--------------------------+----------+-----------+--------+----------+\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "table2 = PrettyTable()\n",
        "table2.field_names = [\"Classification algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        "table2.add_row([\"Random Forest\", rfac, rfpc, rfrc, rff])\n",
        "table2.add_row([\"KNN\", knnac, knnpc, knnrc, knnf])\n",
        "table2.add_row([\"SVM\",svmac, svmpc, svmrc, svmf ])\n",
        "table2.add_row([\"LogisticRegression\",lrac, lrpc, lrrc, lrf ])\n",
        "table2.title = 'Results using GridSearchCV'\n",
        "print(table2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52OPWPD2afi"
      },
      "source": [
        "#Task 3. \n",
        "The dataset consists of **2000 user-created movie reviews** archived on the IMDb(Internet Movie Database). The reviews are equally partitioned into a positive set and a negative set (1000+1000). Each review consists of a plain text file (.txt) and a class label representing the overall user opinion. \n",
        "The class attribute has only two values: **pos** (positive) or **neg** (negative).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDcxOQRmDz_h"
      },
      "source": [
        "*   3.1 Importing additional libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjyW06skDwvL",
        "outputId": "0adfd869-bde6-47c7-ee72-8831bcd499d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk, random\n",
        "nltk.download('movie_reviews')#download movie reviews dataset\n",
        "from nltk.corpus import movie_reviews\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJpsTIiyv-1h"
      },
      "source": [
        "*   3.2. Movie reviews information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZE7A0Au1Pg0",
        "outputId": "f19da75b-928f-4cdf-c910-61b13e641617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "['neg', 'pos']\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
            "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "print(len(movie_reviews.fileids()))\n",
        "print(movie_reviews.categories())\n",
        "print(movie_reviews.words()[:100])\n",
        "print(movie_reviews.fileids()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pHmMpqMHS23"
      },
      "source": [
        "*   3.3. Create dataset from movie reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "45aY6woMHSH5"
      },
      "outputs": [],
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.seed(123)\n",
        "random.shuffle(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNke0Da5HqFa",
        "outputId": "b5567830-d988-49a8-91a1-0018c4ce2255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Reviews/Documents: 2000\n",
            "Corpus Size (words): 1583820\n",
            "Sample Text of Doc 1:\n",
            "------------------------------\n",
            "most movies seem to release a third movie just so it can be called a trilogy . rocky iii seems to kind of fit in that category , but manages to be slightly unique . the rocky formula of \" rocky loses fight / rocky trains / rocky wins fight\n"
          ]
        }
      ],
      "source": [
        "print('Number of Reviews/Documents: {}'.format(len(documents)))\n",
        "print('Corpus Size (words): {}'.format(np.sum([len(d) for (d,l) in documents])))\n",
        "print('Sample Text of Doc 1:')\n",
        "print('-'*30)\n",
        "print(' '.join(documents[0][0][:50])) # first 50 words of the first document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVFUEhnXHsGd",
        "outputId": "6a079a8a-a8cc-45e0-be4d-0008285c8166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'pos': 1000, 'neg': 1000})\n"
          ]
        }
      ],
      "source": [
        "sentiment_distr = Counter([label for (words, label) in documents])\n",
        "print(sentiment_distr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXiEbMzHgVC"
      },
      "source": [
        "*   3.4. Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v_-0gZZFHvJN"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(documents, test_size = 0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUGlm5TGHvpV",
        "outputId": "1c818b57-4892-4ce7-e6e6-72f2ef862d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'neg': 615, 'pos': 585})\n",
            "Counter({'pos': 415, 'neg': 385})\n"
          ]
        }
      ],
      "source": [
        "## Sentiment Distrubtion for Train and Test\n",
        "print(Counter([label for (words, label) in train]))\n",
        "print(Counter([label for (words, label) in test]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l1ppl_0RHx1P"
      },
      "outputs": [],
      "source": [
        "X_train = [' '.join(words) for (words, label) in train]\n",
        "X_test = [' '.join(words) for (words, label) in test]\n",
        "y_train = [label for (words, label) in train]\n",
        "y_test = [label for (words, label) in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUaXrjxH6Ee"
      },
      "source": [
        "*   3.5. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fzwM0nsIH-8l"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "X_train_bow = tfidf_vec.fit_transform(X_train) # fit train\n",
        "X_test_bow = tfidf_vec.transform(X_test) # transform test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP1vB3loIF28"
      },
      "source": [
        "*   3.6. Apply **SVM** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3FHQqh1Hlrd",
        "outputId": "20059b8a-2029-4113-c6cb-6b842f17bdb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.77      0.82      0.79       385\n",
            "         pos       0.82      0.78      0.80       415\n",
            "\n",
            "    accuracy                           0.80       800\n",
            "   macro avg       0.80      0.80      0.80       800\n",
            "weighted avg       0.80      0.80      0.80       800\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'C': 1, 'kernel': 'linear'}\n",
            "SVC(C=1, kernel='linear')\n",
            "0.8291666666666666\n",
            "Accuracy :  0.8\n",
            "Precision :  0.8\n",
            "Recall score :  0.8\n",
            "F1 score :  0.8\n"
          ]
        }
      ],
      "source": [
        "# code\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train_bow, y_train)\n",
        "predictions = svc.predict(X_test_bow)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(SVC(), param_grid, refit=True, scoring = 'accuracy')\n",
        "clf.fit(X_train_bow, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "y_pred = svc.predict(X_test_bow)\n",
        "svmac = round(accuracy_score(y_test, y_pred), 2)\n",
        "svmpc = round(precision_score(y_test, y_pred, average='micro'), 2)\n",
        "svmrc= round(recall_score(y_test, y_pred, average='micro'), 2)\n",
        "svmf= round(f1_score(y_test, y_pred, average='micro'), 2)\n",
        "print (\"Accuracy : \", svmac) \n",
        "print (\"Precision : \", svmpc) \n",
        "print (\"Recall score : \", svmrc) \n",
        "print (\"F1 score : \", svmf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Fy8jYBIdxi"
      },
      "source": [
        "*   3.7. Apply **RandomForest** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyfw2R-gIhWl",
        "outputId": "7d886963-4ca8-452d-d598-98a70a46067a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.72      0.85      0.78       385\n",
            "         pos       0.83      0.69      0.76       415\n",
            "\n",
            "    accuracy                           0.77       800\n",
            "   macro avg       0.78      0.77      0.77       800\n",
            "weighted avg       0.78      0.77      0.77       800\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'C': 1, 'kernel': 'linear'}\n",
            "SVC(C=1, kernel='linear')\n",
            "0.8291666666666666\n",
            "Accuracy :  0.7675\n",
            "Precision :  0.7675\n",
            "Recall score :  0.7675\n",
            "F1 score :  0.7675\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "rfcsf=RandomForestClassifier()\n",
        "#Train the model using the training sets\n",
        "rfcsf.fit(X_train_bow,y_train)\n",
        "y_pred = rfcsf.predict(X_test_bow)\n",
        "print(classification_report(y_test, y_pred))\n",
        "CV_rfc = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv= 5)\n",
        "CV_rfc.fit(X_train_bow, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "rfac = round(accuracy_score(y_test, y_pred), 4)\n",
        "rfpc = round(precision_score(y_test, y_pred, average='micro'), 4)\n",
        "rfrc= round(recall_score(y_test, y_pred, average='micro'), 4)\n",
        "rff= round(f1_score(y_test, y_pred, average='micro'), 4)\n",
        "print (\"Accuracy : \", rfac) \n",
        "print (\"Precision : \", rfpc) \n",
        "print (\"Recall score : \", rfrc) \n",
        "print (\"F1 score : \", rff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btsVKjIIiLT"
      },
      "source": [
        "*   3.8. Apply **kNN** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZmFu1ZQImhn",
        "outputId": "f9d50176-e167-47e0-f5d7-b177b9252d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.69      0.42      0.52       385\n",
            "         pos       0.61      0.83      0.70       415\n",
            "\n",
            "    accuracy                           0.63       800\n",
            "   macro avg       0.65      0.62      0.61       800\n",
            "weighted avg       0.65      0.63      0.61       800\n",
            "\n",
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_n_neighbors', 'param_p', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'n_neighbors': 7, 'p': 2}\n",
            "KNeighborsClassifier(n_neighbors=7)\n",
            "0.6283333333333333\n",
            "kNN Classifier:\n",
            "Accuracy: 0.6312\n",
            "Precision: 0.6312\n",
            "Recall: 0.6312\n",
            "F1-score: 0.6312\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'p': [1, 2],\n",
        "}\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_bow, y_train)\n",
        "predictions = knn.predict(X_test_bow)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, refit = True)\n",
        "clf.fit(X_train_bow, y_train)\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(clf.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "y_pred_knn =  knn.predict(X_test_bow)\n",
        "knnac =round(accuracy_score(y_test, y_pred_knn), 4)\n",
        "knnpc =round(precision_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnrc=round(recall_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "knnf=round(f1_score(y_test, y_pred_knn, average = 'micro'), 4)\n",
        "print('kNN Classifier:')\n",
        "print('Accuracy:', knnac)\n",
        "print('Precision:', knnpc)\n",
        "print('Recall:', knnrc)\n",
        "print('F1-score:', knnf)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ix_HeVGIvDu"
      },
      "source": [
        "*   3.9. Apply **LogisticRegression** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTd3alCMIr-i",
        "outputId": "08aef0a4-b5bb-43be-dd71-a41a72c4c143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 10, 'max_iter': 100}\n",
            "LogisticRegression(C=10)\n",
            "KNeighborsClassifier(n_neighbors=7)\n",
            "0.6283333333333333\n",
            "Logistic Regression Classifier:\n",
            "Accuracy: 0.79\n",
            "Precision: 0.79\n",
            "Recall: 0.79\n",
            "F1-score: 0.79\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "clf_lr = LogisticRegression()\n",
        "clf_lr.fit(X_train_bow, y_train)\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'max_iter': [100, 500, 1000, 2000]\n",
        "}\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, refit= True)\n",
        "grid_search.fit(X_train_bow, y_train)\n",
        "sorted(grid_search.cv_results_.keys())\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid_search.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid_search.best_estimator_)\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "y_pred_lr = clf_lr.predict(X_test_bow)\n",
        "lrac = round(accuracy_score(y_test, y_pred_lr), 2)\n",
        "lrpc = round(precision_score(y_test, y_pred_lr, average = 'micro'), 2)\n",
        "lrrc= round(recall_score(y_test, y_pred_lr, average = 'micro'), 2)\n",
        "lrf= round(f1_score(y_test, y_pred_lr, average = 'micro'), 2)\n",
        "print('Logistic Regression Classifier:')\n",
        "print('Accuracy:', lrac)\n",
        "print('Precision:', lrpc)\n",
        "print('Recall:', lrrc)\n",
        "print('F1-score:', lrf)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhYF2y6eI058"
      },
      "source": [
        "*   3.10. Compare the best obtained results among classification algorithms (use PrettyTable to dispaly the results) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSrWP8VD9JPJ",
        "outputId": "fb619c9f-08ce-4334-cd9b-097ad039e4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+----------+-----------+--------+----------+\n",
            "| Classification algorithm | Accuracy | Precision | Recall | F1-score |\n",
            "+--------------------------+----------+-----------+--------+----------+\n",
            "|      Random Forest       |  0.7675  |   0.7675  | 0.7675 |  0.7675  |\n",
            "|           KNN            |  0.6312  |   0.6312  | 0.6312 |  0.6312  |\n",
            "|           SVM            |   0.8    |    0.8    |  0.8   |   0.8    |\n",
            "|    LogisticRegression    |   0.79   |    0.79   |  0.79  |   0.79   |\n",
            "+--------------------------+----------+-----------+--------+----------+\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "table3 = PrettyTable()\n",
        "table3.field_names = [\"Classification algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        "table3.add_row([\"Random Forest\", rfac, rfpc, rfrc, rff])\n",
        "table3.add_row([\"KNN\", knnac, knnpc, knnrc, knnf])\n",
        "table3.add_row([\"SVM\",svmac, svmpc, svmrc, svmf ])\n",
        "table3.add_row([\"LogisticRegression\",lrac, lrpc, lrrc, lrf ])\n",
        "table3.title = 'Results using GridSearchCV'\n",
        "print(table3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok7RGkea_b7n"
      },
      "source": [
        "#Finally,\n",
        "Save a copy in your Github. Remember renaming the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}